{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58fd0ed4",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 17px\">\n",
    "    <b> TASK 1 </b>\n",
    "</div >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f7aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient \n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "590ee896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_nltk(): \n",
    "    \"\"\"need to download these once before running any of their related methods on a computer\"\"\" \n",
    "    nltk.download(\"stopwords\") #used for stopwords \n",
    "    nltk.download(\"punkt\") #used for tonkenising"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b11056",
   "metadata": {},
   "source": [
    "*CURATING DATA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36f4a5a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\61469\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\61469\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def remove_digits(text):\n",
    "    \"\"\"returns words without numbers\"\"\"\n",
    "    return re.sub(r\"[\\d]\", \" \", text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    \"\"\"returns words without URL links\"\"\"\n",
    "    text = \" \".join(x for x in text.split() if not (x.startswith(\"https:\") or x.startswith(\"www.\")))\n",
    "    text = re.sub(r'[\\w]+.com', \" \", text)\n",
    "    return text\n",
    "\n",
    "def remove_tagging(text):\n",
    "    \"\"\"returns words without taggings\"\"\"\n",
    "    return \" \".join(x for x in text.split() if not x.startswith(\"@\"))\n",
    "\n",
    "def remove_non_alphabetical(text):\n",
    "    \"\"\"returns words without non-alphanumeric characters such as emojis and punctuations\"\"\"\n",
    "    return re.sub(r\"[^\\w]\",\" \",text)\n",
    "\n",
    "def remove_email(text):\n",
    "    \"\"\"returns words without email addresses\"\"\"\n",
    "    return re.sub(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", \" \", text)\n",
    "\n",
    "def remove_non_english_words(text):\n",
    "    \"\"\"returns words without non-english terms\"\"\"\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    return \" \".join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())\n",
    "\n",
    "def remove_single_character(text):\n",
    "    \"\"\"returns words without single letters such as 'r', 'e', 'h'\"\"\"\n",
    "    return \" \".join(x for x in text.split() if len(x) > 1)\n",
    "    \n",
    "def join_single_space(text):\n",
    "    \"\"\"returns single-spaced words\"\"\"\n",
    "    text = text.strip()\n",
    "    return \" \".join(word_tokenize((text)))\n",
    "\n",
    "def curate_body(text):\n",
    "    text = remove_email(text)\n",
    "    text = remove_URL(text)\n",
    "    text = remove_tagging(text)\n",
    "    text = remove_digits(text)\n",
    "    text = remove_non_english_words(text)\n",
    "    text = remove_non_alphabetical(text)\n",
    "    text = remove_single_character(text)\n",
    "    text = join_single_space(text)\n",
    "    return text\n",
    "    \n",
    "#accesses database and its collection '10000 tweets' \n",
    "client = MongoClient(\"localhost:27017\")\n",
    "db = client[\"Assignment_1\"]\n",
    "\n",
    "download_nltk() #only necessary on first time the script is run, left in to demonstrate\n",
    "\n",
    "for t in db[\"10000 tweets\"].find({},{\"body\": 1,\"_id\": 1}):\n",
    "    curated = curate_body(t[\"body\"])\n",
    "    db[\"10000 tweets\"].update_many({\"_id\": t[\"_id\"]}, {\"$set\":{\"curated body\": str(curated)}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf938fff",
   "metadata": {},
   "source": [
    "*EXTRACTING KEYWORDS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "834cae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words): \n",
    "    \"\"\"returns list of words without inconsequetial/unimportant/common words\"\"\" \n",
    "    result = [] \n",
    "    for w in words: \n",
    "        if w not in stopwords.words(\"english\"): \n",
    "            result.append(w) \n",
    "    return result \n",
    "\n",
    "def find_keywords_in_text(text): \n",
    "    words = word_tokenize(text) #convert into list of words \n",
    "    keywords = remove_stopwords(words) #remove stopwords \n",
    "    return keywords \n",
    "\n",
    "def keywords_to_csv(keywords): \n",
    "    \"\"\"takes in a list of keywords and converts to CSV format\"\"\" \n",
    "    csv: str = \"\" \n",
    "    for index, keyword in enumerate(keywords): \n",
    "        if index != (len(keywords) - 1):\n",
    "            csv = csv + keyword + \",\"\n",
    "        else:\n",
    "            csv = csv + keyword\n",
    "    return csv\n",
    "\n",
    "#Loops through each tweet, calls various functions to help in extracting keywords, \n",
    "#then updates the database with the keywords in CSV format \n",
    "for t in db[\"10000 tweets\"].find({}, {\"curated body\": 1, \"_id\": 1}):\n",
    "    keywords = find_keywords_in_text(t[\"curated body\"])\n",
    "    csv = keywords_to_csv(keywords)\n",
    "    db[\"10000 tweets\"].update_many({\"_id\": t[\"_id\"]}, {\"$set\": {\"keyword\": str(csv)}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fe7c68",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 17px\">\n",
    "    <b> TASK 2 </b>\n",
    "</div >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9d190f",
   "metadata": {},
   "source": [
    "*CURATING DATA - PREPROCESSING*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aac4386",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in db[\"10000 tweets\"].find({},{\"curated body\": 1, \"_id\": 0}):\n",
    "    with open('curated_body_tweet.txt', 'a+', encoding='utf-8') as f:\n",
    "        f.write(str(t[\"curated body\"]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d1032",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 17px\">\n",
    "    <b> TASK 3 </b>\n",
    "</div >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eeafbe",
   "metadata": {},
   "source": [
    "*CURATING DATA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc90410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_timezone(text):\n",
    "    \"\"\"returns words without timezone terms, i.e. 'Pacific Time', that bear little location meaning in actor.twitterTimeZone\"\"\"\n",
    "    return re.sub(r'[\\w]+[\\s]+Time', \" \", text)\n",
    "\n",
    "def capitalise_words(text):\n",
    "    \"\"\"returns all words capitalised for downstream location name identification\"\"\"\n",
    "    return text.title()\n",
    "\n",
    "def curate_location(text):\n",
    "    text = remove_email(text)\n",
    "    text = remove_URL(text)\n",
    "    text = remove_tagging(text)\n",
    "    text = remove_timezone(text)\n",
    "    text = remove_digits(text)\n",
    "    text = remove_non_alphabetical(text)\n",
    "    text = remove_single_character(text)\n",
    "    text = capitalise_words(text)\n",
    "    text = join_single_space(text)\n",
    "    return text\n",
    "\n",
    "#accesses database and its collection '10000 tweets' \n",
    "client = MongoClient(\"localhost:27017\")\n",
    "db = client[\"Assignment_1\"]\n",
    "\n",
    "for t in db[\"10000 tweets\"].find({},{\"actor\": 1, \"_id\": 1}):\n",
    "    if (db[\"10000 tweets\"].count_documents({\"$and\": [{\"_id\": t[\"_id\"]}, {\"actor.twitterTimeZone\": {\"$exists\": \"true\"}}]}) > 0) or (db[\"10000 tweets\"].count_documents({\"$and\": [{\"_id\": t[\"_id\"]}, {\"actor.location.displayName\": {\"$exists\": \"true\"}}]}) > 0):\n",
    "        if (db[\"10000 tweets\"].count_documents({\"$and\": [{\"_id\": t[\"_id\"]}, {\"actor.twitterTimeZone\": {\"$exists\": \"true\"}}]}) > 0) and (db[\"10000 tweets\"].count_documents({\"$and\": [{\"_id\": t[\"_id\"]}, {\"actor.location.displayName\": {\"$exists\": \"true\"}}]}) > 0):\n",
    "            curated_timezone = curate_location(str(t[\"actor\"][\"twitterTimeZone\"]))\n",
    "            curated_displayName = curate_location(str(t[\"actor\"][\"location\"][\"displayName\"]))\n",
    "        elif (db[\"10000 tweets\"].count_documents({\"$and\": [{\"_id\": t[\"_id\"]}, {\"actor.twitterTimeZone\": {\"$exists\": \"true\"}}]}) == 0) and (db[\"10000 tweets\"].count_documents({\"$and\": [{\"_id\": t[\"_id\"]}, {\"actor.location.displayName\": {\"$exists\": \"true\"}}]}) > 0):\n",
    "            curated_timezone = \"None\"\n",
    "            curated_displayName = curate_location(str(t[\"actor\"][\"location\"][\"displayName\"]))\n",
    "        elif (db[\"10000 tweets\"].count_documents({\"$and\": [{\"_id\": t[\"_id\"]}, {\"actor.twitterTimeZone\": {\"$exists\": \"true\"}}]}) > 0) and (db[\"10000 tweets\"].count_documents({\"$and\": [{\"_id\": t[\"_id\"]}, {\"actor.location.displayName\": {\"$exists\": \"true\"}}]}) == 0): \n",
    "            curated_timezone = curate_location(str(t[\"actor\"][\"twitterTimeZone\"]))\n",
    "            curated_displayName = \"None\"\n",
    "        db[\"10000 tweets\"].update_many({\"_id\":t[\"_id\"]},{\"$set\":{\"curated twitterTimeZone\": str(curated_timezone)}})\n",
    "        db[\"10000 tweets\"].update_many({\"_id\":t[\"_id\"]},{\"$set\":{\"curated displayName\": str(curated_displayName)}})\n",
    "        db[\"10000 tweets\"].update_many({\"_id\":t[\"_id\"]},{\"$set\":{\"curated location\": str(curated_timezone) + \" , \" + str(curated_displayName)}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c10ad2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in db[\"10000 tweets\"].find({},{\"curated location\": 1, \"_id\": 0}):\n",
    "    with open('tweet_city.txt', 'a+', encoding='utf-8') as f:\n",
    "        f.write(str(t[\"curated location\"] + \"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bcb986",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 17px\">\n",
    "    <b> TASK 4 </b>\n",
    "</div >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e77d0b2",
   "metadata": {},
   "source": [
    "*EXPLORING DATA*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9798ed0d",
   "metadata": {},
   "source": [
    "A brief analysis of the \"object.id\" values reveals that most of them have \"object:search.twitter.com,2005:\" or \"tag:search.twitter.com,2005:\". As this information does not contribute to the sorting of the \"object.id\" values, we can check whether all 10000 tweets' object id have this characteristic for the sake of removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7b7fb09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db[\"10000 tweets\"].count_documents({\"object.id\" : {\"$regex\": \":search.twitter.com,2005:\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e97e7",
   "metadata": {},
   "source": [
    "As the output shows that all 10000 tweets have either \"\"object:search.twitter.com,2005:\" or \"tag:search.twitter.com,2005:\", we can curate the \"object.id\" values by removing them for ease of downstream sorting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db83c19",
   "metadata": {},
   "source": [
    "*CURATING DATA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4405932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_id_non_essentials(text):\n",
    "    \"\"\"returns object ids without 'object:search.twitter.com,2005:' or 'tag:search.twitter.com,2005:'\"\"\"\n",
    "    return re.sub(r'[\\w]+:search.twitter.com,2005:', \"\", text)\n",
    "\n",
    "for t in db[\"10000 tweets\"].find({},{\"object\": 1, \"_id\": 1}):\n",
    "    curated = remove_id_non_essentials(t[\"object\"][\"id\"])\n",
    "    db[\"10000 tweets\"].update_many({\"_id\":t[\"_id\"]},{\"$set\":{\"curated object id\": str(curated)}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55d6bd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in db[\"10000 tweets\"].find({}, {\"curated object id\": 1, \"_id\": 0}):\n",
    "    with open('curated_tweet_object_id.txt', 'a+', encoding='utf-8') as f:\n",
    "        f.write(str(t[\"curated object id\"]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5dc9dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in db[\"10000 tweets\"].find({}, {\"curated object id\": 1, \"_id\": 0}):  \n",
    "    json_string = json.dumps([int(t[\"curated object id\"])])\n",
    "    with open(\"curated_tweet_object_id.json\", \"a+\") as jsonFile:\n",
    "        jsonFile.write(json_string + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89cf05",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 17px\">\n",
    "    <b> TASK 5 </b>\n",
    "</div >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de4172dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in db[\"10000 tweets\"].find({}, {\"curated body\": 1, \"_id\": 0}):  \n",
    "    json_string = json.dumps([str(t[\"curated body\"])])\n",
    "    with open(\"curated_body_tweet.json\", \"a+\") as jsonFile:\n",
    "        jsonFile.write(json_string + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
